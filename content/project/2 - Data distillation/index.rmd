---
title: "Data Distillation Using Quantitative Input Influence"
date: "`r format(Sys.time(), '%d %B, %Y, %H, %M')`"
tags:
- Deep Learning
- Neural network
- Data distillation
comments: yes
---

## Context

In the era of data proliferation, processing and analyzing large amount of data while minimizing resource utilization is becoming an imminent area of research. Although the advancement of technology and storage has improved our abilities to extract meaningful insights from the data that we collect, the introduction of deep learning networks revealed severe limitations in our current abilities to process all data in an effective and efficient manner. This realization prompted many researchers to investigate methodologies that could reduce computation time and resources while retaining the accuracy of our insights. One way to approach this issue is via data distillation. In this paper, we investigated data distillation in the context of deep neural networks using a novel strategy: QII (Quantitative Input Influence) measures with 2019 American Community Survey dataset. We attempted to demonstrate that using inferences drawn from QII measures can successfully extract a smaller dataset that generates similar prediction accuracy when compared to the whole dataset. Although our work has not been completed to fully verify our hypothesis, we were able to show the potential of the QII measures for such use for future work.

## Method

We used [Quantitative Input Influence tool](https://www.andrew.cmu.edu/user/danupam/datta-sen-zick-oakland16.pdf) from [this](https://github.com/cmu-transparency/tool-qii) repository and [train_test_split function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from scikit-learn to perform data distillation on the 2019 American Community Survey data.

## Flowchart
The general process of evaluating our QII-aware sampling is shown in the following figure.

[DL Project-Evaluation](https://user-images.githubusercontent.com/70864873/145465078-c13ae746-f0ed-4f8e-ad49-fd9b356ddafb.png)


## File Explanation
[reference_model.ipynb](exp_QII_features.ipynb)
This notebook implements our baseline MLP model.
It first loads the ACS dataset and does data preprocessing. The preprocessing includes:
1) Drop all data with age under 16 (legal work age)
2) One-hot encoding for categorical data.
3) All NaN values will be replaced by zero. 
4) All flag features that have the same value for more than 99% will be dropped.
5) All features with more than 60% being null value will be dropped.

Then, we send the processed data to the MLP model, and get the plot of R<sup>2</sup> versus epoch.
We also plot the training and testing loss and accuracy.


[exp_QII_features.ipynb](exp_QII_features.ipynb) 
This notebook implements MLP model with the QII-aware features. 
It applies the same data preprocessing methods, and we only take the top N QII features (with highest Shapley values) to train the MLP. 
Then we plot the R<sup>2</sup> versus epoch.
We also plotted the training and testing loss and accuracy.

[exp_Stratification.ipynb](exp_Stratification.ipynb)
This notebook implements MLP model with stratified data points.
It applies the same data preprocessing methods, and we use ``` train_test_split ``` function from ```scikit-learn``` library to split the data.
Then we feed those stratified data into our MLP model, and get the plot of R<sup>2</sup> versus epoch.
We also plotted the training and testing loss and accuracy. 


[QII.ipynb](QII.ipynb)
This notebook implement QII method using functions from [this](https://github.com/cmu-transparency/tool-qii) repository.
It applies the same data preprocessing methods, and then take samples from the original dataset to select most important features.
Since QII functions from the repository require the label to only have 2 categories, we categorize the label in three different way: 
1) Poor vs. Rest
2) Medium vs. Rest
3) Rich vs. Rest

We apply QII method to each of the label-categorized data, and get the most important features for each of them.
Then we can use the most important features to do stratification.

For more detailed explanation of each part of the code, please refer to the section titles and comments inside the notebooks.

## Direction for running
For each notebook, running from top to bottom should give correct output. 
To run the baseline, run [reference_model.ipynb](exp_QII_features.ipynb). 
To apply QII on sampled dataset and get QII features, run [QII.ipynb](QII.ipynb).
To use only top N features as the input to the model, run [exp_QII_features.ipynb](exp_QII_features.ipynb). 
To do data stratification based on the most important feature, run [exp_Stratification.ipynb](exp_Stratification.ipynb).

## Reference

[Quantitative Input Influence Tool](https://github.com/cmu-transparency/tool-qii)
[Anupam Data, Shayak Sen, and Yair Zick Algorithmic Transparency via Quantitative Input Influence: Theory and Experiments with Learning Systems](https://www.andrew.cmu.edu/user/danupam/datta-sen-zick-oakland16.pdf)


** This project's repository can be found on my [github](https://github.com/lilyportfolio/project_QII) **

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```